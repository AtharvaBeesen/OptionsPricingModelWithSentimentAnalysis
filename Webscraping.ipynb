{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "537e407d",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4ea7493",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\users\\swati\\anaconda3\\lib\\site-packages (0.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\swati\\anaconda3\\lib\\site-packages (from bs4) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\swati\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.3.2.post1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9e27b517",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\swati\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\swati\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\swati\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7c175ebc",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument of type 'WordListCorpusReader' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[79], line 60\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m link \u001b[38;5;129;01min\u001b[39;00m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     58\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(link\u001b[38;5;241m.\u001b[39mstring)) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<class \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbs4.element.NavigableString\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(link\u001b[38;5;241m.\u001b[39mstring) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m35\u001b[39m:\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;66;03m# Focus on the text content of the snippet, excluding the link itself (Optional)\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     cleaned_text \u001b[38;5;241m=\u001b[39m \u001b[43mclean_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlink\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstring\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Remove potential leading/trailing whitespace\u001b[39;00m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mM\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(cleaned_text))\n\u001b[0;32m     62\u001b[0m     M \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[79], line 37\u001b[0m, in \u001b[0;36mclean_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Data Preprocessing Enhancements:\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# 1. Filtering (Optional): You can add logic here to filter unwanted entries.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m \n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# 3. Remove Stopwords\u001b[39;00m\n\u001b[0;32m     36\u001b[0m stop_words \u001b[38;5;241m=\u001b[39m stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stopwords]\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# 4. Lemmatization (replace with stemming if preferred)\u001b[39;00m\n\u001b[0;32m     40\u001b[0m lemmatizer \u001b[38;5;241m=\u001b[39m WordNetLemmatizer()\n",
      "Cell \u001b[1;32mIn[79], line 37\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Data Preprocessing Enhancements:\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# 1. Filtering (Optional): You can add logic here to filter unwanted entries.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m \n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# 3. Remove Stopwords\u001b[39;00m\n\u001b[0;32m     36\u001b[0m stop_words \u001b[38;5;241m=\u001b[39m stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstopwords\u001b[49m]\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# 4. Lemmatization (replace with stemming if preferred)\u001b[39;00m\n\u001b[0;32m     40\u001b[0m lemmatizer \u001b[38;5;241m=\u001b[39m WordNetLemmatizer()\n",
      "\u001b[1;31mTypeError\u001b[0m: argument of type 'WordListCorpusReader' is not iterable"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup as BS\n",
    "import requests as req\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def clean_text(text):\n",
    "  \"\"\"\n",
    "  Cleans and preprocesses text data, keeping percentages and numbers.\n",
    "\n",
    "  Args:\n",
    "      text (str): The text to be cleaned.\n",
    "\n",
    "  Returns:\n",
    "      list: A list of cleaned and lemmatized words.\n",
    "  \"\"\"\n",
    "  # Lowercase\n",
    "  text = text.lower()\n",
    "  # Remove HTML tags\n",
    "  text = BS(text, \"html.parser\").get_text(separator=\" \")\n",
    "  # Remove special characters, keeping alphanumeric, whitespace, and percentages\n",
    "  import re\n",
    "  text = re.sub(r\"[^a-zA-Z0-9\\s%]\", \"\", text)\n",
    "  # Tokenize into words, considering percentages and numbers with periods as single tokens\n",
    "  tokens = re.findall(r\"[a-zA-Z]+|\\d+(?:\\.\\d+)?%|\\d+\\.\\d+|[0-9]+\", text)\n",
    "\n",
    "  # Data Preprocessing Enhancements:\n",
    "\n",
    "  # 1. Filtering (Optional): You can add logic here to filter unwanted entries.\n",
    "  #    For example, filter out entries with very short text snippets (less than X words).\n",
    "  #    filtered_tokens = [token for token in tokens if len(token) > 5]  # Example filter\n",
    "\n",
    "  # 2. Remove Links (Optional): If you only want the core text, remove links.\n",
    "  #    tokens = [token for token in tokens if not token.startswith(\"http\")]\n",
    "\n",
    "  # 3. Remove Stopwords\n",
    "  stop_words = stopwords.words(\"english\")\n",
    "  tokens = [token for token in tokens if token not in stopwords]\n",
    "\n",
    "  # 4. Lemmatization (replace with stemming if preferred)\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  stop_words = stopwords.words(\"english\")  # Get stopwords as a list\n",
    "  tokens = [token for token in tokens if token not in stop_words]\n",
    "  return tokens\n",
    "\n",
    "# Define URLs for each website\n",
    "urls = [\n",
    "  \"https://www.businesstoday.in/latest/economy\",\n",
    "  \"https://www.cnbc.com/finance/\",\n",
    "  \"https://seekingalpha.com/\"\n",
    "]\n",
    "\n",
    "# Loop through each URL\n",
    "for url in urls:\n",
    "  webpage = req.get(url)\n",
    "  soup = BS(webpage.content, \"html.parser\")\n",
    "  M = 1\n",
    "  for link in soup.find_all('a'):\n",
    "    if str(type(link.string)) == \"<class 'bs4.element.NavigableString'>\" and len(link.string) > 35:\n",
    "      # Focus on the text content of the snippet, excluding the link itself (Optional)\n",
    "      cleaned_text = clean_text(link.string.strip())  # Remove potential leading/trailing whitespace\n",
    "      print(f\"{M}.\", \" \".join(cleaned_text))\n",
    "      M += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
